{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Reto 3\n",
    "\n",
    "## Miguel Ortiz y Andrea Castiella\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import OurUtils as our\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   altitud  azimut  inclinacion  DH_agua  DV_agua  DH_camino  sombra_9am  \\\n",
      "0     3351     354           19      450       95       1064         185   \n",
      "1     2995      84            7      481       30       5154         231   \n",
      "2     2884      72           25      210      -45        466         239   \n",
      "3     3139      76           11      301       58       1368         234   \n",
      "4     3018     312           17       30       10       1719         172   \n",
      "\n",
      "   sombra_12pm  sombra_3pm  DH_fuego  ...  t31  t32  t33  t34  t35  t36  t37  \\\n",
      "0          203         153       711  ...    0    0    0    0    0    0    0   \n",
      "1          227         129      5488  ...    0    0    0    0    0    0    0   \n",
      "2          183          60      2123  ...    0    0    0    0    0    0    0   \n",
      "3          220         117      3282  ...    0    0    1    0    0    0    0   \n",
      "4          225         193      1961  ...    0    0    0    0    0    0    0   \n",
      "\n",
      "   t38  t39  t40  \n",
      "0    0    1    0  \n",
      "1    0    0    0  \n",
      "2    0    0    0  \n",
      "3    0    0    0  \n",
      "4    0    0    0  \n",
      "\n",
      "[5 rows x 54 columns]\n",
      "   tipo_bosque\n",
      "0            1\n",
      "1            2\n",
      "2            2\n",
      "3            1\n",
      "4            1\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "X_full = pd.read_csv('Reto 3-Dataset/reto3_trainX.csv', header=0, low_memory=False)\n",
    "Y_full = pd.read_csv('Reto 3-Dataset/reto3_trainY.csv', header=0, low_memory=False)\n",
    "\n",
    "print(X_full.head())\n",
    "print(Y_full.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset\n",
    "Train split: 80%\n",
    "\n",
    "Validation split: 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split into train and validation\n",
    "valid_size = 0.2\n",
    "seed = 1234\n",
    "X_train, Y_train, X_valid, Y_valid = \\\n",
    "   our.single_stratified_split(X_full, Y_full, test_size=valid_size, random_state=seed)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "over = SMOTE(random_state=seed, sampling_strategy={3:10000 , 4:10000 , 5: 10000, 6:10000 , 7:10000})\n",
    "X_train, Y_train = over.fit_resample(X_train, Y_train)\n",
    "\n",
    "under = RandomUnderSampler(random_state=seed, sampling_strategy={1:10000 , 2:10000})\n",
    "X_train, Y_train = over.fit_resample(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale and transform features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X_train)\n",
    "poly = PolynomialFeatures(2)\n",
    "X = poly.fit_transform(X)\n",
    "Y = Y_train.values.ravel()\n",
    "#X, pca = our.our_PCA(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnum_of_pc = len(pca.singular_values_)\\nstrTitle = '\\n First %d principal components' %(num_of_pc)\\nprint(strTitle)\\nprint(pca.components_.T)\\nstrTitle = '\\n First %d singular values ' %(num_of_pc)\\nprint(strTitle)\\nprint(pca.singular_values_.T)\\nstrTitle = '\\n Explained Variance Ratio'\\nprint(strTitle)\\nprint(pca.explained_variance_ratio_.T)\\nstrTitle = '\\n Accumulated Explained Variance Ratio'\\nprint(strTitle)\\nprint(np.cumsum(pca.explained_variance_ratio_.T))\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_proy, pca = our.our_PCA(X)\n",
    "# X_proy = our.our_kernelPCA(X)\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# sm = SMOTE(random_state=seed, sampling_strategy=\"minority\")\n",
    "# X_proy, Y = sm.fit_resample(X_proy, Y)\n",
    "\n",
    "# num_of_pc = len(pca.singular_values_)\n",
    "# strTitle = '\\n First %d principal components' %(num_of_pc)\n",
    "# print(strTitle)\n",
    "# print(pca.components_.T)\n",
    "# strTitle = '\\n First %d singular values ' %(num_of_pc)\n",
    "# print(strTitle)\n",
    "# print(pca.singular_values_.T)\n",
    "# strTitle = '\\n Explained Variance Ratio'\n",
    "# print(strTitle)\n",
    "# print(pca.explained_variance_ratio_.T)\n",
    "# strTitle = '\\n Accumulated Explained Variance Ratio'\n",
    "# print(strTitle)\n",
    "# print(np.cumsum(pca.explained_variance_ratio_.T))\n",
    "# X = X_proy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass. OvO vs OvR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OvO produces 21 classifiers\n",
      "OvR produces 7 classifiers\n"
     ]
    }
   ],
   "source": [
    "# Multiclass\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "max_depth = 15\n",
    "base_clf = DecisionTreeClassifier(max_depth = max_depth, random_state=seed)\n",
    "\n",
    "# base_clf = SVC(kernel='rbf', degree=2, gamma=1, random_state = seed)\n",
    "\n",
    "# Train normal\n",
    "clf = base_clf.fit(X,Y)\n",
    "\n",
    "# Train OvO \n",
    "OvO_clf = OneVsOneClassifier(base_clf)\n",
    "OvO_clf.fit(X,Y)\n",
    "\n",
    "# Train OvR \n",
    "OvR_clf = OneVsRestClassifier(base_clf)\n",
    "OvR_clf.fit(X,Y)\n",
    "\n",
    "# Check\n",
    "strlog = \"OvO produces %d classifiers\" %(len(OvO_clf.estimators_))\n",
    "print(strlog)\n",
    "strlog = \"OvR produces %d classifiers\" %(len(OvR_clf.estimators_))\n",
    "print(strlog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Feature engineering, selection and rescaling to [0,1]\n",
    "X_pred = scaler.transform(X_valid)\n",
    "X_pred = poly.fit_transform(X_pred)\n",
    "# PCA\n",
    "#X_pred = pca.transform(X_pred)\n",
    "Y_true = Y_valid.values.ravel()\n",
    "\n",
    "# predict\n",
    "Y_pred = clf.predict_proba(X_pred)\n",
    "\n",
    "# predict\n",
    "Y_pred_OvO = OvO_clf.predict(X_pred)\n",
    "Y_pred_OvR = OvR_clf.predict(X_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "To check the performance of our model, we use Confusion Matrix as metric. This give us a general idea of how good is the model and let us compare it with the multiple combinations of feature that we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OvO confusion matrix:\n",
      "\n",
      "[[6862 1327    2    0   81   13  189]\n",
      " [1076 9621   68    0  395  139   33]\n",
      " [   0   37 1149   42    9  193    0]\n",
      " [   0    0    8   92    0   10    0]\n",
      " [   5   65    5    0  301    4    0]\n",
      " [   0   30   66    8    6  585    0]\n",
      " [  95    6    0    0    0    0  719]]\n",
      "\n",
      "\n",
      "OvO Hits  = 19329\n",
      "OvO Fails = 3912\n",
      "OvO Accuracy = 0.831677\n",
      "OvO Cohen Accuracy = 0.734778\n",
      "\n",
      "OvR confusion matrix:\n",
      "\n",
      "[[7030 1225    7    0   40    9  163]\n",
      " [1052 9854   74    1  221  102   28]\n",
      " [  12   60 1172   31    5  150    0]\n",
      " [   1    1    8   94    0    6    0]\n",
      " [  10   82    6    0  277    4    1]\n",
      " [   8   42   90    9    9  537    0]\n",
      " [  82   16    0    0    1    0  721]]\n",
      "\n",
      "\n",
      "OvR Hits  = 19685\n",
      "OvR Fails = 3556\n",
      "OvR Accuracy = 0.846995\n",
      "OvR Cohen Accuracy = 0.756496\n"
     ]
    }
   ],
   "source": [
    "# Performance metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "conf_mat_OvO = confusion_matrix(Y_true,Y_pred_OvO)\n",
    "hits_OvO = np.trace(conf_mat_OvO)\n",
    "conf_mat_OvR = confusion_matrix(Y_true,Y_pred_OvR)\n",
    "hits_OvR = np.trace(conf_mat_OvR)\n",
    "\n",
    "# Print out\n",
    "print(\"\\nOvO confusion matrix:\\n\")\n",
    "print(conf_mat_OvO)\n",
    "print(\"\\n\")\n",
    "print( \"OvO Hits  = %d\"%(hits_OvO) ) \n",
    "print( \"OvO Fails = %d\"%(Y_true.shape[0]-hits_OvO) )\n",
    "print( \"OvO Accuracy = %f\"%(hits_OvO/(Y_true.shape[0])))\n",
    "print( \"OvO Cohen Accuracy = %f\"%(cohen_kappa_score(Y_true, Y_pred_OvO)))\n",
    "#print( \"OvO AuC = \", (roc_auc_score(Y_true, Y_pred, multi_class=\"ovo\")))\n",
    "\n",
    "print(\"\\nOvR confusion matrix:\\n\")\n",
    "print(conf_mat_OvR)\n",
    "print( \"\\n\")\n",
    "print( \"OvR Hits  = %d\"%(hits_OvR) ) \n",
    "print( \"OvR Fails = %d\"%(Y_true.shape[0]-hits_OvR) )\n",
    "print( \"OvR Accuracy = %f\"%(hits_OvR/Y_true.shape[0]))\n",
    "print( \"OvR Cohen Accuracy = %f\"%(cohen_kappa_score(Y_true, Y_pred_OvR)))\n",
    "#print( \"OvR AuC = \", (roc_auc_score(Y_true, Y_pred, multi_class=\"ovr\")))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
