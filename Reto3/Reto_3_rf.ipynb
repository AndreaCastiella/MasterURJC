{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Reto 3\n",
    "\n",
    "## Miguel Ortiz y Andrea Castiella\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import OurUtils as our\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   altitud  azimut  inclinacion  DH_agua  DV_agua  DH_camino  sombra_9am  \\\n",
      "0     3351     354           19      450       95       1064         185   \n",
      "1     2995      84            7      481       30       5154         231   \n",
      "2     2884      72           25      210      -45        466         239   \n",
      "3     3139      76           11      301       58       1368         234   \n",
      "4     3018     312           17       30       10       1719         172   \n",
      "\n",
      "   sombra_12pm  sombra_3pm  DH_fuego  ...  t31  t32  t33  t34  t35  t36  t37  \\\n",
      "0          203         153       711  ...    0    0    0    0    0    0    0   \n",
      "1          227         129      5488  ...    0    0    0    0    0    0    0   \n",
      "2          183          60      2123  ...    0    0    0    0    0    0    0   \n",
      "3          220         117      3282  ...    0    0    1    0    0    0    0   \n",
      "4          225         193      1961  ...    0    0    0    0    0    0    0   \n",
      "\n",
      "   t38  t39  t40  \n",
      "0    0    1    0  \n",
      "1    0    0    0  \n",
      "2    0    0    0  \n",
      "3    0    0    0  \n",
      "4    0    0    0  \n",
      "\n",
      "[5 rows x 54 columns]\n",
      "   tipo_bosque\n",
      "0            1\n",
      "1            2\n",
      "2            2\n",
      "3            1\n",
      "4            1\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "X_full = pd.read_csv('Reto 3-Dataset/reto3_trainX.csv', header=0, low_memory=False)\n",
    "Y_full = pd.read_csv('Reto 3-Dataset/reto3_trainY.csv', header=0, low_memory=False)\n",
    "\n",
    "print(X_full.head())\n",
    "print(Y_full.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset\n",
    "Train split: 80%\n",
    "\n",
    "Validation split: 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split into train and validation\n",
    "valid_size = 0.2\n",
    "seed = 1234\n",
    "X_train, Y_train, X_valid, Y_valid = \\\n",
    "   our.single_stratified_split(X_full, Y_full, test_size=valid_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbalanced class problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "over = SMOTE(random_state=seed, sampling_strategy={3:10000 , 4:10000 , 5: 10000, 6:10000 , 7:10000})\n",
    "X_train, Y_train = over.fit_resample(X_train, Y_train)\n",
    "\n",
    "under = RandomUnderSampler(random_state=seed, sampling_strategy={1:10000 , 2:10000})\n",
    "X_train, Y_train = over.fit_resample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.iloc[:,[23, 0, 13, 5, 25, 35, 51]]\n",
    "# X_valid = X_valid.iloc[:,[23, 0, 13, 5, 25, 35, 51]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale and transform features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## train\n",
    "X, Y, scaler, pca, transformer = our.our_scale_transform_features(X_train,Y_train)\n",
    "\n",
    "## val\n",
    "X_pred, Y_true, scaler, pca, transformer = our.our_scale_transform_features(X_valid,Y_valid, scaler=scaler, transform=transformer)\n",
    "\n",
    "# # scale features\n",
    "# scaler = MinMaxScaler()\n",
    "# X = scaler.fit_transform(X_train)\n",
    "\n",
    "# # pca\n",
    "# pca_bool = False\n",
    "\n",
    "# if pca_bool:\n",
    "#     X, pca = our.our_PCA(X)\n",
    "\n",
    "# # apply some preproccesing\n",
    "# procc_type = \"transform\"\n",
    "\n",
    "# if procc_type == \"poly\":\n",
    "#     poly = PolynomialFeatures(2)\n",
    "#     X = poly.fit_transform(X)\n",
    "\n",
    "# if procc_type == \"transform\":\n",
    "#     transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "#     X = transformer.fit_transform(X)\n",
    "\n",
    "# # get Y values\n",
    "# Y = Y_train.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92961, 4)\n",
      "\n",
      " First 4 principal components\n",
      "[[ 0.54657452  0.07371693  0.04353025  0.41961752]\n",
      " [-0.32583588  0.08312472 -0.1163221   0.05576592]\n",
      " [ 0.66396056  0.07417547  0.08131946  0.1662541 ]\n",
      " [-0.36001774 -0.05669879  0.01621619  0.88298115]\n",
      " [-0.0573745  -0.5166215   0.83708605 -0.0244898 ]\n",
      " [-0.13463679  0.84361369  0.51143895 -0.00989523]\n",
      " [-0.05667427 -0.01890801 -0.12415964  0.11321439]]\n",
      "\n",
      " First 4 singular values \n",
      "[96.83252594 71.72809213 66.34651326 62.59299545]\n",
      "\n",
      " Explained Variance Ratio\n",
      "[0.32648582 0.17914321 0.1532703  0.1364185 ]\n",
      "\n",
      " Accumulated Explained Variance Ratio\n",
      "[0.32648582 0.50562904 0.65889934 0.79531784]\n"
     ]
    }
   ],
   "source": [
    "# X_proy, pca = our.our_PCA(X)\n",
    "# num_of_pc = len(pca.singular_values_)\n",
    "# strTitle = '\\n First %d principal components' %(num_of_pc)\n",
    "# print(strTitle)\n",
    "# print(pca.components_.T)\n",
    "# strTitle = '\\n First %d singular values ' %(num_of_pc)\n",
    "# print(strTitle)\n",
    "# print(pca.singular_values_.T)\n",
    "# strTitle = '\\n Explained Variance Ratio'\n",
    "# print(strTitle)\n",
    "# print(pca.explained_variance_ratio_.T)\n",
    "# strTitle = '\\n Accumulated Explained Variance Ratio'\n",
    "# print(strTitle)\n",
    "# print(np.cumsum(pca.explained_variance_ratio_.T))\n",
    "# X = X_proy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass. OvO vs OvR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OvO produces 21 classifiers\n",
      "OvR produces 7 classifiers\n"
     ]
    }
   ],
   "source": [
    "# Multiclass\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_estimators = 150\n",
    "max_leaf_nodes = 20\n",
    "max_depth = 2\n",
    "\n",
    "#weights = {1:.25, 2:.25, 3:.7, 4:1 , 5:.7 ,6:.7 , 7:.7 }\n",
    "base_clf = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                max_depth=max_depth)#, class_weight=weights)\n",
    "                                #max_leaf_nodes=max_leaf_nodes)\n",
    "\n",
    "\n",
    "\n",
    "base_clf.fit(X,Y)\n",
    "\n",
    "# Train OvO \n",
    "OvO_clf = OneVsOneClassifier(base_clf)\n",
    "OvO_clf.fit(X,Y)\n",
    "\n",
    "# Train OvR \n",
    "OvR_clf = OneVsRestClassifier(base_clf)\n",
    "OvR_clf.fit(X,Y)\n",
    "\n",
    "# Check\n",
    "strlog = \"OvO produces %d classifiers\" %(len(OvO_clf.estimators_))\n",
    "print(strlog)\n",
    "strlog = \"OvR produces %d classifiers\" %(len(OvR_clf.estimators_))\n",
    "print(strlog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Feature engineering, selection and rescaling to [0,1]\n",
    "# X_pred = scaler.transform(X_valid)\n",
    "# # X_pred = poly.fit_transform(X_pred)\n",
    "# X_pred = transformer.fit_transform(X_pred)\n",
    "# # PCA\n",
    "# #X_pred = pca.transform(X_pred)\n",
    "# Y_true = Y_valid.values.ravel()\n",
    "\n",
    "# predict\n",
    "Y_pred = base_clf.predict_proba(X_pred)\n",
    "\n",
    "# predict\n",
    "Y_pred_OvO = OvO_clf.predict(X_pred)\n",
    "Y_pred_OvR = OvR_clf.predict(X_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "To check the performance of our model, we use Confusion Matrix as metric. This give us a general idea of how good is the model and let us compare it with the multiple combinations of feature that we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OvO confusion matrix:\n",
      "\n",
      "[[ 4094  4380     0     0     0     0     0]\n",
      " [ 1066 10170    17     3     0    76     0]\n",
      " [    0   610   262   252     0   306     0]\n",
      " [    0     0     1   102     0     7     0]\n",
      " [    0   380     0     0     0     0     0]\n",
      " [    0   299    82    56     0   258     0]\n",
      " [  747    73     0     0     0     0     0]]\n",
      "\n",
      "\n",
      "OvO Hits  = 14886\n",
      "OvO Fails = 8355\n",
      "OvO Accuracy = 0.640506\n",
      "\n",
      "OvR confusion matrix:\n",
      "\n",
      "OvO Cohen Accuracy = 0.371103\n",
      "[[ 2057  6348     0     0     0     0    69]\n",
      " [  522 10733     4    13     0    58     2]\n",
      " [    0   652    76   427     0   275     0]\n",
      " [    0     1     0   109     0     0     0]\n",
      " [    0   380     0     0     0     0     0]\n",
      " [    0   330    32   163     0   170     0]\n",
      " [  198   413     0     0     0     0   209]]\n",
      "\n",
      "\n",
      "OvR Hits  = 13354\n",
      "OvR Fails = 9887\n",
      "OvR Accuracy = 0.574588\n",
      "OvR Cohen Accuracy = 0.371103\n"
     ]
    }
   ],
   "source": [
    "# Performance metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "conf_mat_OvO = confusion_matrix(Y_true,Y_pred_OvO)\n",
    "hits_OvO = np.trace(conf_mat_OvO)\n",
    "conf_mat_OvR = confusion_matrix(Y_true,Y_pred_OvR)\n",
    "hits_OvR = np.trace(conf_mat_OvR)\n",
    "\n",
    "# Print out\n",
    "print(\"\\nOvO confusion matrix:\\n\")\n",
    "print(conf_mat_OvO)\n",
    "print(\"\\n\")\n",
    "print( \"OvO Hits  = %d\"%(hits_OvO) ) \n",
    "print( \"OvO Fails = %d\"%(Y_true.shape[0]-hits_OvO) )\n",
    "print( \"OvO Accuracy = %f\"%(hits_OvO/(Y_true.shape[0])))\n",
    "print(\"\\nOvR confusion matrix:\\n\")\n",
    "print( \"OvO Cohen Accuracy = %f\"%(cohen_kappa_score(Y_true, Y_pred_OvO)))\n",
    "print(conf_mat_OvR)\n",
    "print( \"\\n\")\n",
    "print( \"OvR Hits  = %d\"%(hits_OvR) ) \n",
    "print( \"OvR Fails = %d\"%(Y_true.shape[0]-hits_OvR) )\n",
    "print( \"OvR Accuracy = %f\"%(hits_OvR/Y_true.shape[0]))\n",
    "print( \"OvR Cohen Accuracy = %f\"%(cohen_kappa_score(Y_true, Y_pred_OvO)))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
